{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze\n",
    "In this demonstration, we analyze entity resolution as a downstreaming application. We show that the tables integrated using ALITE prepares a better ground for entity resolution than the tables integrated using outer join operator. The results are reported in the form of precision, recall and F-score of entity resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('python version: ' + sys.version )\n",
    "print('pandas version: ' + pd.__version__ )\n",
    "print('magellan version: ' + em.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'integration-result/alite_fd_stadiums_0.csv'\n",
    "#file = 'integration-result/outerjoin_stadiums_0.csv'\n",
    "path_A = 'em_inputs' + os.sep + file\n",
    "path_B = 'em_inputs' + os.sep + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = em.read_csv_metadata(path_A, key = \"index\")\n",
    "B = em.read_csv_metadata(path_B, key = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tuples in A: ' + str(len(A)))\n",
    "print('Number of tuples in B: ' + str(len(B)))\n",
    "print('Number of tuples in A X B (i.e the cartesian product): ' + str(len(A)*len(B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize overlap blocker\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "ob = em.OverlapBlocker()\n",
    "# Block over title attribute\n",
    "attrs = ['player', 'team', 'stadium']\n",
    "\n",
    "C = ob.block_tables(A, B, 'stadium', 'stadium', \n",
    "                    l_output_attrs=attrs,\n",
    "                    r_output_attrs=attrs, \n",
    "                    show_progress=False, overlap_size=2)\n",
    "len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = em.sample_table(C, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f = em.get_features_for_matching(A, B, validate_inferred_attr_types = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brm = em.BooleanRuleMatcher()\n",
    "rule_1 = ['player_player_jac_dlm_dc0_dlm_dc0(ltuple, rtuple) > 0.3', 'player_player_cos_dlm_dc0_dlm_dc0(ltuple, rtuple) > 0.3', 'player_player_lev_sim(ltuple, rtuple) > 0.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rule_name = brm.add_rule(rule_1, match_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = brm.predict(table=C, target_attr='predicted_labels', inplace=True)\n",
    "C['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CC  = C[C['ltable_index'] != C['rtable_index']]\n",
    "CCC = CC[['ltable_index', 'rtable_index', 'predictions']]\n",
    "final_cols = list(CCC.columns)\n",
    "final_rows = []\n",
    "indexed_rows = set()\n",
    "for index, rows in CCC.iterrows():\n",
    "    ltable = rows['ltable_index']\n",
    "    rtable = rows['rtable_index']\n",
    "    predictions = rows['predictions']\n",
    "    if (ltable, rtable) not in indexed_rows and (rtable, ltable) not in indexed_rows:\n",
    "        final_rows.append((ltable, rtable, predictions))\n",
    "        indexed_rows.add((ltable, rtable))\n",
    "final_dataframe = pd.DataFrame(final_rows, columns= final_cols )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.to_csv(\"em_outputs/em_result_\"+file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_tuples = set()\n",
    "\n",
    "for index, row in final_dataframe.iterrows():\n",
    "    if row['predictions'] == 1:\n",
    "        remove_tuples.add(min(row['ltable_index'], row['rtable_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_cleaned = A[~A.index.isin(remove_tuples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltable_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare groundtruth for comparison. Since the partitioned tables may not have complete information, we only include those columns for the evaluation whose information are available (the columns participating on joins.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_table = pd.read_csv(r\"data/em_gold/em_stadium_gold_complete_dirty.csv\")\n",
    "dirty_groundtruth = set()\n",
    "for index, row in gt_table.iterrows():\n",
    "    player = row['Player'].lower()\n",
    "    team = row['Team'].lower()\n",
    "    facility = row['Stadium'].lower()\n",
    "    dirty_groundtruth.add((player, team, facility))\n",
    "print(\"the dirty groundtruth size is:\", len(dirty_groundtruth))\n",
    "\n",
    "gt_table = pd.read_csv(r\"data/em_gold/em_stadium_gold_complete.csv\")\n",
    "clean_groundtruth = set()\n",
    "for index, row in gt_table.iterrows():\n",
    "    player = row['Player'].lower()\n",
    "    team = row['Team'].lower()\n",
    "    facility = row['Stadium'].lower()\n",
    "    clean_groundtruth.add((player, team, facility))\n",
    "print(\"the clean groundtruth size is:\", len(clean_groundtruth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare integrated table for comparison for both dirty (A) and clean (ltable_cleaned) tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integration_result_dirty = set()\n",
    "for index, row in A.iterrows():\n",
    "    player = row['player']\n",
    "    team = row['team']\n",
    "    facility = row['stadium']\n",
    "    integration_result_dirty.add((player, team, facility))\n",
    "print(\"The dirty result size is:\", len(integration_result_dirty))\n",
    "\n",
    "integration_result_clean = set()\n",
    "for index, row in ltable_cleaned.iterrows():\n",
    "    player = row['player']\n",
    "    team = row['team']\n",
    "    facility = row['stadium']\n",
    "    integration_result_clean.add((player, team, facility))\n",
    "print(\"The clean result size is:\", len(integration_result_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dirty table size |T|: \", A.shape[0])\n",
    "print(\"Clean table size |T|: \", ltable_cleaned.shape[0])\n",
    "print(\"Clean table intersection with clean ground truth |T int T*|: \", len(clean_groundtruth.intersection(integration_result_clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(clean_groundtruth.intersection(integration_result_clean))/ len(integration_result_clean)\n",
    "recall = len(clean_groundtruth.intersection(integration_result_clean)) / len(clean_groundtruth)\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print (\"F1-score = \", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0822af77c0478f28fc75a22a78f781852ff3b3e7f1600f0f7be807f0cd90bde5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
